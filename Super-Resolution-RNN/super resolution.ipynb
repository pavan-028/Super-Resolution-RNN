{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-09-29T20:04:50.342857Z",
     "iopub.status.busy": "2022-09-29T20:04:50.342259Z",
     "iopub.status.idle": "2022-09-29T20:04:52.193494Z",
     "shell.execute_reply": "2022-09-29T20:04:52.192351Z",
     "shell.execute_reply.started": "2022-09-29T20:04:50.342729Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D, add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_directory = '/mnt/DATA/jas123/Downloads/img_super/archive/folder/'\n",
    "hires_folder = os.path.join(base_directory, 'high res')\n",
    "lowres_folder = os.path.join(base_directory, 'low res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>low_res</th>\n",
       "      <th>high_res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/DATA/jas123/Downloads/img_super/archive/f...</td>\n",
       "      <td>/mnt/DATA/jas123/Downloads/img_super/archive/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/DATA/jas123/Downloads/img_super/archive/f...</td>\n",
       "      <td>/mnt/DATA/jas123/Downloads/img_super/archive/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/DATA/jas123/Downloads/img_super/archive/f...</td>\n",
       "      <td>/mnt/DATA/jas123/Downloads/img_super/archive/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/DATA/jas123/Downloads/img_super/archive/f...</td>\n",
       "      <td>/mnt/DATA/jas123/Downloads/img_super/archive/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/DATA/jas123/Downloads/img_super/archive/f...</td>\n",
       "      <td>/mnt/DATA/jas123/Downloads/img_super/archive/f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             low_res  \\\n",
       "0  /mnt/DATA/jas123/Downloads/img_super/archive/f...   \n",
       "1  /mnt/DATA/jas123/Downloads/img_super/archive/f...   \n",
       "2  /mnt/DATA/jas123/Downloads/img_super/archive/f...   \n",
       "3  /mnt/DATA/jas123/Downloads/img_super/archive/f...   \n",
       "4  /mnt/DATA/jas123/Downloads/img_super/archive/f...   \n",
       "\n",
       "                                            high_res  \n",
       "0  /mnt/DATA/jas123/Downloads/img_super/archive/f...  \n",
       "1  /mnt/DATA/jas123/Downloads/img_super/archive/f...  \n",
       "2  /mnt/DATA/jas123/Downloads/img_super/archive/f...  \n",
       "3  /mnt/DATA/jas123/Downloads/img_super/archive/f...  \n",
       "4  /mnt/DATA/jas123/Downloads/img_super/archive/f...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/mnt/DATA/jas123/Downloads/img_super/archive/folder/image_data.csv\")\n",
    "data['low_res'] = data['low_res'].apply(lambda x: os.path.join(lowres_folder,x))\n",
    "data['high_res'] = data['high_res'].apply(lambda x: os.path.join(hires_folder,x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3198 validated image filenames.\n",
      "Found 3198 validated image filenames.\n",
      "Found 564 validated image filenames.\n",
      "Found 564 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "image_datagen = ImageDataGenerator(rescale=1./255,validation_split=0.15)\n",
    "mask_datagen = ImageDataGenerator(rescale=1./255,validation_split=0.15)\n",
    "\n",
    "train_hiresimage_generator = image_datagen.flow_from_dataframe(\n",
    "        data,\n",
    "        x_col='high_res',\n",
    "        target_size=(512, 512),\n",
    "        class_mode = None,\n",
    "        batch_size = batch_size,\n",
    "        seed=42,\n",
    "        subset='training')\n",
    "\n",
    "train_lowresimage_generator = image_datagen.flow_from_dataframe(\n",
    "        data,\n",
    "        x_col='low_res',\n",
    "        target_size=(128, 128),\n",
    "        class_mode = None,\n",
    "        batch_size = batch_size,\n",
    "        seed=42,\n",
    "        subset='training')\n",
    "\n",
    "val_hiresimage_generator = image_datagen.flow_from_dataframe(\n",
    "        data,\n",
    "        x_col='high_res',\n",
    "        target_size=(512, 512),\n",
    "        class_mode = None,\n",
    "        batch_size = batch_size,\n",
    "        seed=42,\n",
    "        subset='validation')\n",
    "\n",
    "val_lowresimage_generator = image_datagen.flow_from_dataframe(\n",
    "        data,\n",
    "        x_col='low_res',\n",
    "        target_size=(128, 128),\n",
    "        class_mode = None,\n",
    "        batch_size = batch_size,\n",
    "        seed=42,\n",
    "        subset='validation')\n",
    "\n",
    "train_generator = zip(train_lowresimage_generator, train_hiresimage_generator)\n",
    "val_generator = zip(val_lowresimage_generator, val_hiresimage_generator)\n",
    "\n",
    "def imageGenerator(train_generator):\n",
    "    for (low_res, hi_res) in train_generator:\n",
    "            yield (low_res, hi_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSIZE =256\n",
    "LSIZE =64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128, 128, 3)\n",
      "(None, 64, 64, 128)\n",
      "(None, 32, 32, 128)\n",
      "(None, 16, 16, 256)\n",
      "(None, 8, 8, 512)\n",
      "(None, 4, 4, 512)\n",
      "(None, 8, 8, 512)\n",
      "(None, 16, 16, 256)\n",
      "(None, 32, 32, 128)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " sequential_36 (Sequential)  (None, 64, 64, 128)          3584      ['input_5[0][0]']             \n",
      "                                                                                                  \n",
      " sequential_37 (Sequential)  (None, 32, 32, 128)          147584    ['sequential_36[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_38 (Sequential)  (None, 16, 16, 256)          296192    ['sequential_37[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_39 (Sequential)  (None, 8, 8, 512)            1182208   ['sequential_38[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_40 (Sequential)  (None, 4, 4, 512)            2361856   ['sequential_39[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_41 (Sequential)  (None, 8, 8, 512)            2359808   ['sequential_40[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenat  (None, 8, 8, 1024)           0         ['sequential_41[0][0]',       \n",
      " e)                                                                  'sequential_39[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_42 (Sequential)  (None, 16, 16, 256)          2359552   ['concatenate_12[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenat  (None, 16, 16, 512)          0         ['sequential_42[0][0]',       \n",
      " e)                                                                  'sequential_38[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_43 (Sequential)  (None, 32, 32, 128)          589952    ['concatenate_13[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenat  (None, 32, 32, 256)          0         ['sequential_43[0][0]',       \n",
      " e)                                                                  'sequential_37[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_44 (Sequential)  (None, 64, 64, 128)          295040    ['concatenate_14[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenat  (None, 64, 64, 256)          0         ['sequential_44[0][0]',       \n",
      " e)                                                                  'sequential_36[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_45 (Sequential)  (None, 128, 128, 3)          6915      ['concatenate_15[0][0]']      \n",
      "                                                                                                  \n",
      " sequential_46 (Sequential)  (None, 256, 256, 3)          84        ['sequential_45[0][0]']       \n",
      "                                                                                                  \n",
      " sequential_47 (Sequential)  (None, 512, 512, 3)          84        ['sequential_46[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 512, 512, 3)          39        ['sequential_47[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9602898 (36.63 MB)\n",
      "Trainable params: 9600338 (36.62 MB)\n",
      "Non-trainable params: 2560 (10.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSIZE = 128\n",
    "HSIZE = 512\n",
    "from keras import layers\n",
    "import visualkeras\n",
    "def down(filters , kernel_low_size, apply_batch_normalization = True):\n",
    "    downsample = tf.keras.models.Sequential()\n",
    "    downsample.add(layers.Conv2D(filters,kernel_low_size,padding = 'same', strides = 2))\n",
    "    if apply_batch_normalization:\n",
    "        downsample.add(layers.BatchNormalization())\n",
    "    downsample.add(keras.layers.LeakyReLU())\n",
    "    return downsample\n",
    "\n",
    "\n",
    "def up(filters, kernel_low_size, dropout = False):\n",
    "    upsample = tf.keras.models.Sequential()\n",
    "    upsample.add(layers.Conv2DTranspose(filters, kernel_low_size,padding = 'same', strides = 2))\n",
    "    if dropout:\n",
    "        upsample.dropout(0.2)\n",
    "    upsample.add(keras.layers.LeakyReLU())\n",
    "    return upsample\n",
    "\n",
    "def model():\n",
    "    inputs = layers.Input(shape= [LSIZE,LSIZE,3])\n",
    "    print(inputs.shape)\n",
    "    d1 = down(128,(3,3),False)(inputs)\n",
    "    print(d1.shape)\n",
    "    d2 = down(128,(3,3),False)(d1)\n",
    "    print(d2.shape)\n",
    "    d3 = down(256,(3,3),True)(d2)\n",
    "    print(d3.shape)\n",
    "    d4 = down(512,(3,3),True)(d3)\n",
    "    print(d4.shape)\n",
    "    \n",
    "    d5 = down(512,(3,3),True)(d4)\n",
    "    print(d5.shape)\n",
    "    #upsampling\n",
    "    u1 = up(512,(3,3),False)(d5)\n",
    "    print(u1.shape)\n",
    "    u1 = layers.concatenate([u1,d4])\n",
    "    u2 = up(256,(3,3),False)(u1)\n",
    "    print(u2.shape)\n",
    "    u2 = layers.concatenate([u2,d3])\n",
    "    u3 = up(128,(3,3),False)(u2)\n",
    "    print(u3.shape)\n",
    "    u3 = layers.concatenate([u3,d2])\n",
    "    u4 = up(128,(3,3),False)(u3)\n",
    "    u4 = layers.concatenate([u4,d1])\n",
    "    u5 = up(3,(3,3),False)(u4)\n",
    "    u6 = up(3,(3,3),False)(u5)\n",
    "    u7 = up(3,(3,3),False)(u6)\n",
    "    output = layers.Conv2D(3,(2,2),strides = 1, padding = 'same')(u7)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model = model()\n",
    "visualkeras.layered_view(model, draw_volume=False, legend=True, to_file='output1.png').show()\n",
    "model.summary()\n",
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_samples = train_hiresimage_generator.samples\n",
    "val_samples = val_hiresimage_generator.samples\n",
    "\n",
    "train_img_gen = imageGenerator(train_generator)\n",
    "val_image_gen = imageGenerator(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3198/3198 [==============================] - 437s 136ms/step - loss: 0.0573 - acc: 0.6502 - val_loss: 0.0407 - val_acc: 0.7619\n",
      "Epoch 2/20\n",
      "3198/3198 [==============================] - 391s 122ms/step - loss: 0.0425 - acc: 0.7433 - val_loss: 0.0385 - val_acc: 0.7506\n",
      "Epoch 3/20\n",
      "3198/3198 [==============================] - 391s 122ms/step - loss: 0.0412 - acc: 0.7482 - val_loss: 0.0410 - val_acc: 0.7368\n",
      "Epoch 4/20\n",
      "3198/3198 [==============================] - 389s 122ms/step - loss: 0.0403 - acc: 0.7509 - val_loss: 0.0375 - val_acc: 0.7589\n",
      "Epoch 5/20\n",
      "3198/3198 [==============================] - 391s 122ms/step - loss: 0.0394 - acc: 0.7517 - val_loss: 0.0394 - val_acc: 0.7505\n",
      "Epoch 6/20\n",
      "3198/3198 [==============================] - 391s 122ms/step - loss: 0.0388 - acc: 0.7527 - val_loss: 0.0368 - val_acc: 0.7656\n",
      "Epoch 7/20\n",
      "3198/3198 [==============================] - 390s 122ms/step - loss: 0.0381 - acc: 0.7535 - val_loss: 0.0369 - val_acc: 0.7619\n",
      "Epoch 8/20\n",
      "3198/3198 [==============================] - 391s 122ms/step - loss: 0.0378 - acc: 0.7534 - val_loss: 0.0376 - val_acc: 0.7545\n",
      "Epoch 9/20\n",
      "3198/3198 [==============================] - 400s 125ms/step - loss: 0.0374 - acc: 0.7581 - val_loss: 0.0359 - val_acc: 0.7493\n",
      "Epoch 10/20\n",
      "3198/3198 [==============================] - 390s 122ms/step - loss: 0.0370 - acc: 0.7572 - val_loss: 0.0369 - val_acc: 0.7378\n",
      "Epoch 11/20\n",
      "3198/3198 [==============================] - 389s 122ms/step - loss: 0.0368 - acc: 0.7578 - val_loss: 0.0361 - val_acc: 0.7660\n",
      "Epoch 12/20\n",
      "3198/3198 [==============================] - 391s 122ms/step - loss: 0.0365 - acc: 0.7569 - val_loss: 0.0374 - val_acc: 0.7470\n",
      "Epoch 13/20\n",
      "3198/3198 [==============================] - 389s 122ms/step - loss: 0.0363 - acc: 0.7588 - val_loss: 0.0358 - val_acc: 0.7421\n",
      "Epoch 14/20\n",
      "3198/3198 [==============================] - 389s 122ms/step - loss: 0.0361 - acc: 0.7597 - val_loss: 0.0370 - val_acc: 0.7350\n",
      "Epoch 15/20\n",
      "3198/3198 [==============================] - 390s 122ms/step - loss: 0.0360 - acc: 0.7587 - val_loss: 0.0357 - val_acc: 0.7314\n",
      "Epoch 16/20\n",
      "3198/3198 [==============================] - 390s 122ms/step - loss: 0.0358 - acc: 0.7602 - val_loss: 0.0352 - val_acc: 0.7703\n",
      "Epoch 17/20\n",
      "3198/3198 [==============================] - 391s 122ms/step - loss: 0.0357 - acc: 0.7621 - val_loss: 0.0355 - val_acc: 0.7695\n",
      "Epoch 18/20\n",
      "3198/3198 [==============================] - 388s 121ms/step - loss: 0.0346 - acc: 0.7799 - val_loss: 0.0311 - val_acc: 0.8295\n",
      "Epoch 19/20\n",
      "3198/3198 [==============================] - 387s 121ms/step - loss: 0.0302 - acc: 0.8566 - val_loss: 0.0300 - val_acc: 0.8751\n",
      "Epoch 20/20\n",
      "3198/3198 [==============================] - 390s 122ms/step - loss: 0.0295 - acc: 0.8662 - val_loss: 0.0292 - val_acc: 0.8338\n"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'mean_absolute_error',\n",
    "              metrics = ['acc'])\n",
    "H = model.fit(train_img_gen,\n",
    "                    steps_per_epoch=train_samples//batch_size,\n",
    "                    validation_data=val_image_gen,\n",
    "                    validation_steps=val_samples//batch_size,\n",
    "                    epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "for i,m in val_generator:\n",
    "    img,mask = i,m\n",
    "    sr1 = model.predict(img)\n",
    "    if n < 20:\n",
    "        fig, axs = plt.subplots(1 , 3, figsize=(20,4))\n",
    "        axs[0].imshow(img[0])\n",
    "        axs[0].set_title('Low Resolution Image')\n",
    "        axs[1].imshow(mask[0])\n",
    "        axs[1].set_title('High Resolution Image')\n",
    "        axs[2].imshow(sr1[0])\n",
    "        axs[2].set_title('Predicted High Resolution Image')\n",
    "        plt.show()\n",
    "        n+=1\n",
    "    else:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
